{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import gensim\t#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import lucem_illud_2020\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas as pd #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "import spacy\n",
    "import copy\n",
    "import nltk\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('donald_full.pkl', 'rb') as f:\n",
    "    donald_full = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the Submission files\n",
    "donald_sub = pd.read_csv('../project_data/sub_The_Donald-2015-06-16-2018-11-08.csv')\n",
    "\n",
    "# Adding the pre-fix for submissions. Source: https://www.reddit.com/dev/api/\n",
    "donald_sub['id'] = \"t3_\" + donald_sub['id']\n",
    "donald_sub.set_index('id', inplace = True)\n",
    "\n",
    "# Combining title & body of submission  \n",
    "donald_sub['body'] = donald_sub['title'].fillna('') + ' ' + donald_sub['selftext'].fillna('')\n",
    "# donald_sub.head()\n",
    "\n",
    "# Reading in the Comment files\n",
    "# donald_comm = pd.read_csv('../project_data/comm_The_Donald-2015-06-16-2018-11-08.csv')\n",
    "\n",
    "# # Adding the prefix for comments. Source: https://www.reddit.com/dev/api/\n",
    "# donald_comm['id'] = \"t1_\" + donald_comm['id']\n",
    "# donald_comm.set_index('id', inplace = True)\n",
    "# # donald_comm.head()\n",
    "\n",
    "# # Appending the two sets of files\n",
    "# donald_full = donald_sub.append(donald_comm)\n",
    "# # donald_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        14\n",
       "1        28\n",
       "2       178\n",
       "3       875\n",
       "4      6826\n",
       "5     12380\n",
       "6     10172\n",
       "7      7919\n",
       "8     10802\n",
       "9     11673\n",
       "10    14139\n",
       "11    13947\n",
       "12    13896\n",
       "13    14000\n",
       "14    13907\n",
       "15    14093\n",
       "16    13938\n",
       "17    13987\n",
       "18    14007\n",
       "19    14000\n",
       "20    14000\n",
       "21    14000\n",
       "22    13674\n",
       "23    13957\n",
       "24    14048\n",
       "25    13974\n",
       "26    14003\n",
       "27    14000\n",
       "28     3000\n",
       "Name: created_tranche, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "nweeks = 6\n",
    "tstamp_win = nweeks*7*24*60*60\n",
    "nbins = math.ceil((max(donald_sub['created_utc']) - (min(donald_sub['created_utc'])-1))/tstamp_win)\n",
    "bin_list = range((min(donald_sub['created_utc'])-1), min(donald_sub['created_utc'])-1+((nbins+1)*tstamp_win), tstamp_win)\n",
    "donald_sub['created_tranche'] = pd.cut(donald_sub['created_utc'], bin_list, labels = list(range(nbins)))\n",
    "donald_sub['created_tranche'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import LIKE_URL\n",
    "\n",
    "# Add custom infix for '[', ']' to deal with markdown-style hyperlinks\n",
    "\n",
    "infixes = list(nlp.Defaults.infixes)\n",
    "infixes.extend([\"\\\\[\",\"\\\\]\"])\n",
    "infixes\n",
    "\n",
    "infix_regex = spacy.util.compile_infix_regex(infixes)\n",
    "\n",
    "# infix_regex\n",
    "\n",
    "# nlp.tokenizer.\n",
    "\n",
    "nlp.tokenizer.infix_finditer = infix_regex.finditer\n",
    "\n",
    "# Customized word_tokenize function from lucem_illud to remove URL tokens, and save them in new column\n",
    "def word_tokenize_cust(word_list, model=nlp, MAX_LEN=1500000):\n",
    "    \n",
    "    tokenized = []\n",
    "    url_list = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            if token.like_url:\n",
    "                url_list.append(token.text)\n",
    "            else:\n",
    "                tokenized.append(token.text)\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e5cdc855d2437d8ce2fc03b57285ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=315437.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "donald_sub['normalized_sents'] = donald_sub['body'].progress_apply(lambda x: [lucem_illud_2020.normalizeTokens(word_tokenize_cust(s)) for s in lucem_illud_2020.sent_tokenize(str(x))])\n",
    "\n",
    "with open('donald_sub.pkl', 'wb') as fout:\n",
    "    dill.dump(donald_sub, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import itertools \n",
    "\n",
    "sub_ngram_dict3 = {}\n",
    "\n",
    "def snapModelsVec3(groupdf, ngram_dict, sort = True):\n",
    "    n = 2\n",
    "        \n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, groupdf['normalized_sents'].sum())\n",
    "    model = nltk.lm.models.KneserNeyInterpolated(order = n)\n",
    "    model.fit(train_data, padded_sents)\n",
    "    print(\"Model Vocab size: {}\".format(len(model.vocab)), end = '\\n')\n",
    "    model_name = 'sub_ngram_' + str(n) + '_tranche_' + str(groupdf['created_tranche'][0]) + '_KNModel.pkl'\n",
    "    with open(model_name, 'wb') as fout:\n",
    "        dill.dump(model, fout)\n",
    "    ngram_dict[(groupdf['created_tranche'][0], n)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11495d92e3a244d983bc0d7eac0b2d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Vocab size: 342\n",
      "Model Vocab size: 310\n",
      "Model Vocab size: 1812\n",
      "Model Vocab size: 4301\n",
      "Model Vocab size: 13229\n",
      "Model Vocab size: 15918\n",
      "Model Vocab size: 13030\n",
      "Model Vocab size: 13986\n",
      "Model Vocab size: 16249\n",
      "Model Vocab size: 15802\n",
      "Model Vocab size: 22331\n",
      "Model Vocab size: 19207\n",
      "Model Vocab size: 18604\n",
      "Model Vocab size: 21218\n",
      "Model Vocab size: 19275\n",
      "Model Vocab size: 19132\n",
      "Model Vocab size: 19992\n",
      "Model Vocab size: 19789\n",
      "Model Vocab size: 18985\n",
      "Model Vocab size: 19230\n",
      "Model Vocab size: 19034\n",
      "Model Vocab size: 18745\n",
      "Model Vocab size: 17760\n",
      "Model Vocab size: 18584\n",
      "Model Vocab size: 18099\n",
      "Model Vocab size: 17005\n",
      "Model Vocab size: 15699\n",
      "Model Vocab size: 16337\n",
      "Model Vocab size: 7105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "donald_sub.groupby('created_tranche').progress_apply(snapModelsVec3, ngram_dict = sub_ngram_dict3)\n",
    "\n",
    "with open('sub_2gram_model_dict.pkl', 'wb') as fout:\n",
    "    dill.dump(sub_ngram_dict3, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import everygrams, bigrams\n",
    "\n",
    "def ce_calc(tranche_val, bigram_list):\n",
    "    try:\n",
    "        return sub_ngram_dict3[(tranche_val, 2)].entropy(bigram_list)\n",
    "    except ZeroDivisionError:\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234574"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donald_sub_select = donald_sub[(donald_sub['num_comments'] >= 10) & (donald_sub['score'] >= 100) & (donald_sub['created_tranche'] > 4)]\n",
    "len(donald_sub_select)\n",
    "# donald_sub[(donald_sub['num_comments'] >= 10) & (donald_sub['score'] >= 100)]['created_tranche'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "5      8931\n",
       "6      8509\n",
       "7      6180\n",
       "8      7930\n",
       "9      9481\n",
       "10    10382\n",
       "11    10909\n",
       "12    10956\n",
       "13    11468\n",
       "14    11063\n",
       "15    11547\n",
       "16    11849\n",
       "17    11238\n",
       "18    11443\n",
       "19    11297\n",
       "20    10972\n",
       "21    10829\n",
       "22    11369\n",
       "23    11756\n",
       "24    11326\n",
       "25    11306\n",
       "26    11331\n",
       "27     2501\n",
       "28        1\n",
       "Name: created_tranche, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donald_sub_select['created_tranche'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_sub_select['in_toks'] = donald_sub_select['normalized_sents'].progress_apply(lambda x: lucem_illud_2020.normalizeTokens(word_tokenize_cust(x))[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_sub_select['ce_score'] = donald_sub_select.progress_apply(lambda x: ce_calc(x['created_tranche'], list(bigrams(x['in_toks']))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('donald_sub_select.pkl', 'wb') as fout:\n",
    "    dill.dump(donald_sub_select, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Abandoned full-length SLM, with submissions and comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc9aef62ca043db9fce706363855bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Vocab size: 738\n",
      "Model Vocab size: 2811\n",
      "Model Vocab size: 4012\n",
      "Model Vocab size: 10566\n",
      "Model Vocab size: 21558\n",
      "Model Vocab size: 36548\n",
      "Model Vocab size: 40361\n",
      "Model Vocab size: 38636\n",
      "Model Vocab size: 38824\n",
      "Model Vocab size: 38875\n",
      "Model Vocab size: 39239\n",
      "Model Vocab size: 38188\n",
      "Model Vocab size: 40464\n",
      "Model Vocab size: 39463\n",
      "Model Vocab size: 39567\n",
      "Model Vocab size: 40882\n",
      "Model Vocab size: 39375\n",
      "Model Vocab size: 40443\n",
      "Model Vocab size: 39901\n",
      "Model Vocab size: 40265\n",
      "Model Vocab size: 41206\n",
      "Model Vocab size: 40816\n",
      "Model Vocab size: 39905\n",
      "Model Vocab size: 39601\n",
      "Model Vocab size: 40111\n",
      "Model Vocab size: 40309\n",
      "Model Vocab size: 39356\n",
      "Model Vocab size: 39283\n",
      "Model Vocab size: 38002\n",
      "Model Vocab size: 16403\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donald_full.groupby('created_tranche').progress_apply(snapModelsVec3)\n",
    "\n",
    "with open('2gram_model_dict.pkl', 'wb') as fout:\n",
    "    dill.dump(ngram_dict3, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Calling quits after ~12 hours](heatdeath.png)\n",
    "Ran an SLM for ~60 hours, before terminating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_full['ce_2gram'] = np.vectorize(ce_calc)(donald_full['created_tranche'], 2, donald_full['tok_select'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}